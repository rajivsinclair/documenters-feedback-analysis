{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Feedback Analysis with Embeddings and Clustering\n",
    "\n",
    "This notebook demonstrates a complete workflow for analyzing text feedback using modern embedding models and clustering techniques. We'll transform unstructured feedback into actionable insights.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. **Data Loading**: Import and prepare feedback data\n",
    "2. **Embedding Generation**: Convert text to numerical representations\n",
    "3. **Clustering**: Group similar feedback together\n",
    "4. **Analysis**: Understand cluster themes using AI\n",
    "5. **Visualization**: Create interactive visualizations\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "!pip install pandas numpy scikit-learn umap-learn hdbscan matplotlib seaborn plotly google-genai tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Embedding and AI\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "# Clustering and dimensionality reduction\n",
    "import umap\n",
    "import hdbscan\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# Replace with your API key\n",
    "GEMINI_API_KEY = \"YOUR_API_KEY_HERE\"  # Get from https://makersuite.google.com/app/apikey\n",
    "\n",
    "# Model settings\n",
    "EMBEDDING_MODEL = \"gemini-embedding-001\"  # or \"text-embedding-004\"\n",
    "LLM_MODEL = \"gemini-2.0-flash\"\n",
    "\n",
    "# Analysis parameters\n",
    "MIN_FEEDBACK_LENGTH = 50  # Minimum character count\n",
    "BATCH_SIZE = 20  # For API calls\n",
    "MIN_CLUSTER_SIZE = 10  # For HDBSCAN\n",
    "\n",
    "# Initialize Gemini client\n",
    "client = genai.Client(api_key=GEMINI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data (replace with your file path)\n",
    "# Example format: CSV with columns: id, feedback_text, date, category (optional)\n",
    "\n",
    "# For demo, let's create sample data\n",
    "sample_feedback = [\n",
    "    \"The meeting room was difficult to find and parking was a nightmare. Better signage needed.\",\n",
    "    \"Audio quality was terrible - echo made it impossible to understand speakers clearly.\",\n",
    "    \"Great experience documenting the city council meeting. Well organized and easy to follow.\",\n",
    "    \"Meeting was cancelled but no one notified us. Wasted trip downtown.\",\n",
    "    \"The agenda wasn't available until the meeting started, making it hard to prepare.\",\n",
    "    \"Excellent facility with good WiFi and power outlets. Made note-taking much easier.\",\n",
    "    \"Board members spoke too quickly and used lots of acronyms without explanation.\",\n",
    "    \"Meeting ran 2 hours over schedule. Future documenters should plan accordingly.\",\n",
    "    \"First time documenting - the training materials were very helpful!\",\n",
    "    \"Technical issues with the streaming platform made remote attendance frustrating.\"\n",
    "]\n",
    "\n",
    "# Create dataframe\n",
    "df = pd.DataFrame({\n",
    "    'id': range(1, len(sample_feedback) + 1),\n",
    "    'feedback_text': sample_feedback,\n",
    "    'date': pd.date_range('2024-01-01', periods=len(sample_feedback), freq='D'),\n",
    "    'category': ['Logistics'] * 2 + ['Experience'] * 2 + ['Communication'] * 2 + \n",
    "                ['Process'] * 2 + ['Training'] * 1 + ['Technical'] * 1\n",
    "})\n",
    "\n",
    "# For real data, use:\n",
    "# df = pd.read_csv('your_feedback_data.csv')\n",
    "\n",
    "print(f\"Loaded {len(df)} feedback entries\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning and filtering\n",
    "print(\"Data cleaning...\")\n",
    "\n",
    "# Remove null values\n",
    "df = df.dropna(subset=['feedback_text'])\n",
    "\n",
    "# Filter by length\n",
    "df['text_length'] = df['feedback_text'].str.len()\n",
    "df_filtered = df[df['text_length'] >= MIN_FEEDBACK_LENGTH].copy()\n",
    "\n",
    "print(f\"Filtered from {len(df)} to {len(df_filtered)} entries\")\n",
    "print(f\"Average feedback length: {df_filtered['text_length'].mean():.0f} characters\")\n",
    "\n",
    "# Show length distribution\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "df_filtered['text_length'].hist(bins=30, alpha=0.7)\n",
    "plt.xlabel('Feedback Length (characters)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Feedback Length Distribution')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "df_filtered['category'].value_counts().plot(kind='bar', alpha=0.7)\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Feedback by Category')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Embeddings\n",
    "\n",
    "Now we'll convert text to numerical representations using the Gemini embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(texts, batch_size=BATCH_SIZE, task_type=\"CLUSTERING\"):\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of texts using Gemini API.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of strings to embed\n",
    "        batch_size: Number of texts per API call\n",
    "        task_type: One of CLUSTERING, CLASSIFICATION, SEMANTIC_SIMILARITY\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of embeddings\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        \n",
    "        retries = 3\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                # Generate embeddings\n",
    "                result = client.models.embed_content(\n",
    "                    model=EMBEDDING_MODEL,\n",
    "                    contents=batch,\n",
    "                    config=types.EmbedContentConfig(task_type=task_type)\n",
    "                )\n",
    "                \n",
    "                # Extract embedding values\n",
    "                for embedding in result.embeddings:\n",
    "                    embeddings.append(embedding.values)\n",
    "                \n",
    "                # Rate limiting\n",
    "                time.sleep(0.5)\n",
    "                break\n",
    "                \n",
    "            except Exception as e:\n",
    "                if attempt < retries - 1:\n",
    "                    print(f\"\\nRetry {attempt + 1}/{retries} after error: {e}\")\n",
    "                    time.sleep(2 ** attempt)  # Exponential backoff\n",
    "                else:\n",
    "                    print(f\"\\nFailed to generate embeddings for batch {i//batch_size}\")\n",
    "                    # Add zero embeddings for failed batch\n",
    "                    for _ in batch:\n",
    "                        embeddings.append([0] * 3072)  # Default size\n",
    "    \n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Generate embeddings for our feedback\n",
    "embeddings = generate_embeddings(df_filtered['feedback_text'].tolist())\n",
    "print(f\"\\nGenerated embeddings with shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dimensionality Reduction and Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensions for clustering\n",
    "print(\"Reducing dimensions with UMAP...\")\n",
    "\n",
    "# UMAP for clustering (50 dimensions)\n",
    "reducer_clustering = umap.UMAP(\n",
    "    n_components=50,\n",
    "    n_neighbors=15,\n",
    "    min_dist=0.1,\n",
    "    metric='cosine',\n",
    "    random_state=42\n",
    ")\n",
    "embeddings_reduced = reducer_clustering.fit_transform(embeddings)\n",
    "\n",
    "# UMAP for visualization (2 dimensions)\n",
    "reducer_viz = umap.UMAP(\n",
    "    n_components=2,\n",
    "    n_neighbors=15,\n",
    "    min_dist=0.1,\n",
    "    metric='cosine',\n",
    "    random_state=42\n",
    ")\n",
    "embeddings_2d = reducer_viz.fit_transform(embeddings)\n",
    "\n",
    "print(f\"Reduced to {embeddings_reduced.shape[1]} dimensions for clustering\")\n",
    "print(f\"Reduced to {embeddings_2d.shape[1]} dimensions for visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform clustering with HDBSCAN\n",
    "print(\"\\nClustering with HDBSCAN...\")\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=MIN_CLUSTER_SIZE,\n",
    "    min_samples=5,\n",
    "    metric='euclidean',\n",
    "    cluster_selection_epsilon=0.5,\n",
    "    cluster_selection_method='eom'\n",
    ")\n",
    "\n",
    "cluster_labels = clusterer.fit_predict(embeddings_reduced)\n",
    "\n",
    "# Add results to dataframe\n",
    "df_filtered['cluster'] = cluster_labels\n",
    "df_filtered['x'] = embeddings_2d[:, 0]\n",
    "df_filtered['y'] = embeddings_2d[:, 1]\n",
    "\n",
    "# Print cluster statistics\n",
    "n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "n_noise = list(cluster_labels).count(-1)\n",
    "\n",
    "print(f\"\\nNumber of clusters: {n_clusters}\")\n",
    "print(f\"Number of noise points: {n_noise}\")\n",
    "print(\"\\nCluster sizes:\")\n",
    "for cluster_id in sorted(set(cluster_labels)):\n",
    "    if cluster_id != -1:\n",
    "        size = sum(cluster_labels == cluster_id)\n",
    "        print(f\"  Cluster {cluster_id}: {size} points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive scatter plot\n",
    "fig = px.scatter(\n",
    "    df_filtered,\n",
    "    x='x', y='y',\n",
    "    color='cluster',\n",
    "    hover_data=['feedback_text', 'category'],\n",
    "    title='Feedback Clusters Visualization',\n",
    "    labels={'x': 'UMAP 1', 'y': 'UMAP 2'},\n",
    "    color_discrete_map={-1: 'lightgray'}  # Noise points in gray\n",
    ")\n",
    "\n",
    "fig.update_traces(\n",
    "    marker=dict(size=10, line=dict(width=1, color='white')),\n",
    "    selector=dict(mode='markers')\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    width=900,\n",
    "    height=600,\n",
    "    plot_bgcolor='rgba(240,240,240,0.5)'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster size distribution\n",
    "cluster_sizes = df_filtered[df_filtered['cluster'] >= 0]['cluster'].value_counts().sort_index()\n",
    "\n",
    "fig_sizes = go.Figure(data=[\n",
    "    go.Bar(\n",
    "        x=[f'Cluster {i}' for i in cluster_sizes.index],\n",
    "        y=cluster_sizes.values,\n",
    "        text=cluster_sizes.values,\n",
    "        textposition='auto',\n",
    "    )\n",
    "])\n",
    "\n",
    "fig_sizes.update_layout(\n",
    "    title='Cluster Size Distribution',\n",
    "    xaxis_title='Cluster',\n",
    "    yaxis_title='Number of Feedback Entries',\n",
    "    showlegend=False,\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig_sizes.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analyze Cluster Themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_cluster(cluster_feedback, cluster_id):\n",
    "    \"\"\"\n",
    "    Use Gemini to analyze and describe a cluster based on sample feedback.\n",
    "    \"\"\"\n",
    "    # Take representative samples (up to 10)\n",
    "    samples = cluster_feedback[:min(10, len(cluster_feedback))]\n",
    "    \n",
    "    prompt = f\"\"\"Analyze the following {len(samples)} feedback comments that share similar themes:\n",
    "\n",
    "{chr(10).join([f'{i+1}. \"{sample}\"' for i, sample in enumerate(samples)])}\n",
    "\n",
    "Please provide:\n",
    "1. A brief 2-3 sentence description of the main theme or common characteristics\n",
    "2. Key topics or concerns mentioned (bullet points)\n",
    "3. The general tone (positive, negative, neutral, mixed)\n",
    "4. 2-3 actionable insights for improvement\n",
    "\n",
    "Keep the analysis concise and focused on patterns across all samples.\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model=LLM_MODEL,\n",
    "            contents=prompt,\n",
    "            config=types.GenerateContentConfig(\n",
    "                temperature=0.7,\n",
    "                max_output_tokens=500\n",
    "            )\n",
    "        )\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        return f\"Error analyzing cluster: {str(e)}\"\n",
    "\n",
    "# Analyze each cluster\n",
    "cluster_analyses = {}\n",
    "unique_clusters = sorted(set(cluster_labels))\n",
    "unique_clusters = [c for c in unique_clusters if c >= 0]  # Exclude noise\n",
    "\n",
    "print(\"Analyzing clusters...\\n\")\n",
    "for cluster_id in unique_clusters:\n",
    "    cluster_mask = df_filtered['cluster'] == cluster_id\n",
    "    cluster_feedback = df_filtered[cluster_mask]['feedback_text'].tolist()\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"CLUSTER {cluster_id} ({len(cluster_feedback)} entries)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Get analysis\n",
    "    analysis = analyze_cluster(cluster_feedback, cluster_id)\n",
    "    cluster_analyses[cluster_id] = {\n",
    "        'size': len(cluster_feedback),\n",
    "        'analysis': analysis,\n",
    "        'samples': cluster_feedback[:3]\n",
    "    }\n",
    "    \n",
    "    print(analysis)\n",
    "    \n",
    "    # Show sample feedback\n",
    "    print(\"\\nSample feedback:\")\n",
    "    for i, sample in enumerate(cluster_feedback[:3]):\n",
    "        print(f\"{i+1}. {sample[:100]}...\")\n",
    "    \n",
    "    time.sleep(1)  # Rate limiting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = Path('feedback_analysis_output')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save cluster assignments\n",
    "df_filtered.to_csv(output_dir / 'feedback_with_clusters.csv', index=False)\n",
    "print(f\"Saved cluster assignments to {output_dir / 'feedback_with_clusters.csv'}\")\n",
    "\n",
    "# Save cluster analyses\n",
    "with open(output_dir / 'cluster_analyses.json', 'w') as f:\n",
    "    json.dump(cluster_analyses, f, indent=2)\n",
    "print(f\"Saved cluster analyses to {output_dir / 'cluster_analyses.json'}\")\n",
    "\n",
    "# Generate summary report\n",
    "report = f\"\"\"# Feedback Analysis Report\n",
    "\n",
    "**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## Summary Statistics\n",
    "- Total feedback analyzed: {len(df_filtered)}\n",
    "- Number of clusters found: {n_clusters}\n",
    "- Noise points: {n_noise}\n",
    "- Average cluster size: {len(df_filtered[df_filtered['cluster'] >= 0]) / n_clusters:.1f}\n",
    "\n",
    "## Cluster Analyses\n",
    "\"\"\"\n",
    "\n",
    "for cluster_id, data in cluster_analyses.items():\n",
    "    report += f\"\\n### Cluster {cluster_id} (Size: {data['size']})\\n\\n\"\n",
    "    report += data['analysis'] + \"\\n\"\n",
    "\n",
    "# Save report\n",
    "with open(output_dir / 'analysis_report.md', 'w') as f:\n",
    "    f.write(report)\n",
    "print(f\"\\nSaved analysis report to {output_dir / 'analysis_report.md'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Advanced Analysis (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal analysis - how clusters change over time\n",
    "if 'date' in df_filtered.columns:\n",
    "    # Group by month and cluster\n",
    "    df_filtered['month'] = pd.to_datetime(df_filtered['date']).dt.to_period('M')\n",
    "    temporal_data = df_filtered[df_filtered['cluster'] >= 0].groupby(['month', 'cluster']).size().unstack(fill_value=0)\n",
    "    \n",
    "    # Create stacked area chart\n",
    "    fig_temporal = go.Figure()\n",
    "    \n",
    "    for cluster in temporal_data.columns:\n",
    "        fig_temporal.add_trace(go.Scatter(\n",
    "            x=temporal_data.index.astype(str),\n",
    "            y=temporal_data[cluster],\n",
    "            mode='lines',\n",
    "            stackgroup='one',\n",
    "            name=f'Cluster {cluster}'\n",
    "        ))\n",
    "    \n",
    "    fig_temporal.update_layout(\n",
    "        title='Cluster Distribution Over Time',\n",
    "        xaxis_title='Month',\n",
    "        yaxis_title='Number of Feedback Entries',\n",
    "        hovermode='x unified',\n",
    "        height=400\n",
    "    )\n",
    "    \n",
    "    fig_temporal.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-tabulation with categories (if available)\n",
    "if 'category' in df_filtered.columns:\n",
    "    # Create crosstab\n",
    "    crosstab = pd.crosstab(df_filtered['category'], df_filtered['cluster'], normalize='index') * 100\n",
    "    \n",
    "    # Create heatmap\n",
    "    fig_heatmap = px.imshow(\n",
    "        crosstab.values,\n",
    "        labels=dict(x=\"Cluster\", y=\"Category\", color=\"Percentage\"),\n",
    "        x=[f\"Cluster {i}\" if i >= 0 else \"Noise\" for i in crosstab.columns],\n",
    "        y=crosstab.index,\n",
    "        title=\"Category Distribution Across Clusters (%)\",\n",
    "        color_continuous_scale=\"Blues\"\n",
    "    )\n",
    "    \n",
    "    fig_heatmap.update_layout(height=400)\n",
    "    fig_heatmap.show()\n",
    "    \n",
    "    print(\"\\nKey insights:\")\n",
    "    for category in crosstab.index:\n",
    "        dominant_cluster = crosstab.loc[category].idxmax()\n",
    "        if dominant_cluster >= 0:\n",
    "            percentage = crosstab.loc[category, dominant_cluster]\n",
    "            print(f\"- {category} feedback is {percentage:.0f}% in Cluster {dominant_cluster}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusions and Next Steps\n",
    "\n",
    "### What We've Accomplished\n",
    "1. ✅ Converted text feedback into numerical embeddings\n",
    "2. ✅ Clustered similar feedback together\n",
    "3. ✅ Analyzed themes using AI\n",
    "4. ✅ Created visualizations\n",
    "5. ✅ Exported results for further analysis\n",
    "\n",
    "### Next Steps\n",
    "1. **Fine-tune parameters**: Adjust `min_cluster_size` and UMAP parameters\n",
    "2. **Add more data**: Include additional feedback sources\n",
    "3. **Track changes**: Run analysis periodically to track trends\n",
    "4. **Take action**: Use insights to improve your product/service\n",
    "5. **Automate**: Set up scheduled analysis pipelines\n",
    "\n",
    "### Tips for Production Use\n",
    "- Cache embeddings to avoid re-computing\n",
    "- Use batch processing for large datasets\n",
    "- Implement proper error handling\n",
    "- Monitor API costs\n",
    "- Consider fine-tuning embeddings for your domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save this notebook's configuration for reproducibility\n",
    "config = {\n",
    "    'analysis_date': datetime.now().isoformat(),\n",
    "    'parameters': {\n",
    "        'embedding_model': EMBEDDING_MODEL,\n",
    "        'llm_model': LLM_MODEL,\n",
    "        'min_feedback_length': MIN_FEEDBACK_LENGTH,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'min_cluster_size': MIN_CLUSTER_SIZE,\n",
    "        'umap_components': 50,\n",
    "        'umap_neighbors': 15\n",
    "    },\n",
    "    'results': {\n",
    "        'total_feedback': len(df_filtered),\n",
    "        'num_clusters': n_clusters,\n",
    "        'noise_points': n_noise\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(output_dir / 'analysis_config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(\"Analysis complete! 🎉\")\n",
    "print(f\"\\nAll results saved to: {output_dir.absolute()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}