{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Feedback Analysis with Embeddings and Clustering\n",
    "\n",
    "This notebook demonstrates a complete workflow for analyzing text feedback using modern embedding models and clustering techniques. We'll transform unstructured feedback into actionable insights.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. **Data Loading**: Import and prepare feedback data\n",
    "2. **Embedding Generation**: Convert text to numerical representations\n",
    "3. **Clustering**: Group similar feedback together\n",
    "4. **Analysis**: Understand cluster themes using AI\n",
    "5. **Visualization**: Create interactive visualizations\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "!pip install pandas numpy scikit-learn umap-learn hdbscan matplotlib seaborn plotly google-genai tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Embedding and AI\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "# Clustering and dimensionality reduction\n",
    "import umap\n",
    "import hdbscan\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# Replace with your API key\n",
    "GEMINI_API_KEY = \"YOUR_API_KEY_HERE\"  # Get from https://makersuite.google.com/app/apikey\n",
    "\n",
    "# Model settings\n",
    "EMBEDDING_MODEL = \"gemini-embedding-001\"  # or \"text-embedding-004\"\n",
    "LLM_MODEL = \"gemini-2.0-flash\"\n",
    "\n",
    "# Analysis parameters\n",
    "MIN_FEEDBACK_LENGTH = 50  # Minimum character count\n",
    "BATCH_SIZE = 20  # For API calls\n",
    "MIN_CLUSTER_SIZE = 10  # For HDBSCAN\n",
    "\n",
    "# Initialize Gemini client\n",
    "client = genai.Client(api_key=GEMINI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data (replace with your file path)\n",
    "# Example format: CSV with columns: id, feedback_text, date, category (optional)\n",
    "\n",
    "# For demo, let's create sample data\n",
    "sample_feedback = [\n",
    "    \"The meeting room was difficult to find and parking was a nightmare. Better signage needed.\",\n",
    "    \"Audio quality was terrible - echo made it impossible to understand speakers clearly.\",\n",
    "    \"Great experience documenting the city council meeting. Well organized and easy to follow.\",\n",
    "    \"Meeting was cancelled but no one notified us. Wasted trip downtown.\",\n",
    "    \"The agenda wasn't available until the meeting started, making it hard to prepare.\",\n",
    "    \"Excellent facility with good WiFi and power outlets. Made note-taking much easier.\",\n",
    "    \"Board members spoke too quickly and used lots of acronyms without explanation.\",\n",
    "    \"Meeting ran 2 hours over schedule. Future documenters should plan accordingly.\",\n",
    "    \"First time documenting - the training materials were very helpful!\",\n",
    "    \"Technical issues with the streaming platform made remote attendance frustrating.\"\n",
    "]\n",
    "\n",
    "# Create dataframe\n",
    "df = pd.DataFrame({\n",
    "    'id': range(1, len(sample_feedback) + 1),\n",
    "    'feedback_text': sample_feedback,\n",
    "    'date': pd.date_range('2024-01-01', periods=len(sample_feedback), freq='D'),\n",
    "    'category': ['Logistics'] * 2 + ['Experience'] * 2 + ['Communication'] * 2 + \n",
    "                ['Process'] * 2 + ['Training'] * 1 + ['Technical'] * 1\n",
    "})\n",
    "\n",
    "# For real data, use:\n",
    "# df = pd.read_csv('your_feedback_data.csv')\n",
    "\n",
    "print(f\"Loaded {len(df)} feedback entries\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning and filtering\n",
    "print(\"Data cleaning...\")\n",
    "\n",
    "# Remove null values\n",
    "df = df.dropna(subset=['feedback_text'])\n",
    "\n",
    "# Filter by length\n",
    "df['text_length'] = df['feedback_text'].str.len()\n",
    "df_filtered = df[df['text_length'] >= MIN_FEEDBACK_LENGTH].copy()\n",
    "\n",
    "print(f\"Filtered from {len(df)} to {len(df_filtered)} entries\")\n",
    "print(f\"Average feedback length: {df_filtered['text_length'].mean():.0f} characters\")\n",
    "\n",
    "# Show length distribution\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "df_filtered['text_length'].hist(bins=30, alpha=0.7)\n",
    "plt.xlabel('Feedback Length (characters)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Feedback Length Distribution')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "df_filtered['category'].value_counts().plot(kind='bar', alpha=0.7)\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Feedback by Category')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Embeddings\n",
    "\n",
    "Now we'll convert text to numerical representations using the Gemini embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(texts, batch_size=BATCH_SIZE, task_type=\"CLUSTERING\"):\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of texts using Gemini API.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of strings to embed\n",
    "        batch_size: Number of texts per API call\n",
    "        task_type: One of CLUSTERING, CLASSIFICATION, SEMANTIC_SIMILARITY\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of embeddings\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        \n",
    "        retries = 3\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                # Generate embeddings\n",
    "                result = client.models.embed_content(\n",
    "                    model=EMBEDDING_MODEL,\n",
    "                    contents=batch,\n",
    "                    config=types.EmbedContentConfig(task_type=task_type)\n",
    "                )\n",
    "                \n",
    "                # Extract embedding values\n",
    "                for embedding in result.embeddings:\n",
    "                    embeddings.append(embedding.values)\n",
    "                \n",
    "                # Rate limiting\n",
    "                time.sleep(0.5)\n",
    "                break\n",
    "                \n",
    "            except Exception as e:\n",
    "                if attempt < retries - 1:\n",
    "                    print(f\"\\nRetry {attempt + 1}/{retries} after error: {e}\")\n",
    "                    time.sleep(2 ** attempt)  # Exponential backoff\n",
    "                else:\n",
    "                    print(f\"\\nFailed to generate embeddings for batch {i//batch_size}\")\n",
    "                    # Add zero embeddings for failed batch\n",
    "                    for _ in batch:\n",
    "                        embeddings.append([0] * 3072)  # Default size\n",
    "    \n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Generate embeddings for our feedback\n",
    "embeddings = generate_embeddings(df_filtered['feedback_text'].tolist())\n",
    "print(f\"\\nGenerated embeddings with shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dimensionality Reduction and Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensions for clustering\n",
    "print(\"Reducing dimensions with UMAP...\")\n",
    "\n",
    "# UMAP for clustering (50 dimensions)\n",
    "reducer_clustering = umap.UMAP(\n",
    "    n_components=50,\n",
    "    n_neighbors=15,\n",
    "    min_dist=0.1,\n",
    "    metric='cosine',\n",
    "    random_state=42\n",
    ")\n",
    "embeddings_reduced = reducer_clustering.fit_transform(embeddings)\n",
    "\n",
    "# UMAP for visualization (2 dimensions)\n",
    "reducer_viz = umap.UMAP(\n",
    "    n_components=2,\n",
    "    n_neighbors=15,\n",
    "    min_dist=0.1,\n",
    "    metric='cosine',\n",
    "    random_state=42\n",
    ")\n",
    "embeddings_2d = reducer_viz.fit_transform(embeddings)\n",
    "\n",
    "print(f\"Reduced to {embeddings_reduced.shape[1]} dimensions for clustering\")\n",
    "print(f\"Reduced to {embeddings_2d.shape[1]} dimensions for visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform clustering with HDBSCAN\n",
    "print(\"\\nClustering with HDBSCAN...\")\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=MIN_CLUSTER_SIZE,\n",
    "    min_samples=5,\n",
    "    metric='euclidean',\n",
    "    cluster_selection_epsilon=0.5,\n",
    "    cluster_selection_method='eom'\n",
    ")\n",
    "\n",
    "cluster_labels = clusterer.fit_predict(embeddings_reduced)\n",
    "\n",
    "# Add results to dataframe\n",
    "df_filtered['cluster'] = cluster_labels\n",
    "df_filtered['x'] = embeddings_2d[:, 0]\n",
    "df_filtered['y'] = embeddings_2d[:, 1]\n",
    "\n",
    "# Print cluster statistics\n",
    "n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "n_noise = list(cluster_labels).count(-1)\n",
    "\n",
    "print(f\"\\nNumber of clusters: {n_clusters}\")\n",
    "print(f\"Number of noise points: {n_noise}\")\n",
    "print(\"\\nCluster sizes:\")\n",
    "for cluster_id in sorted(set(cluster_labels)):\n",
    "    if cluster_id != -1:\n",
    "        size = sum(cluster_labels == cluster_id)\n",
    "        print(f\"  Cluster {cluster_id}: {size} points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive scatter plot\n",
    "fig = px.scatter(\n",
    "    df_filtered,\n",
    "    x='x', y='y',\n",
    "    color='cluster',\n",
    "    hover_data=['feedback_text', 'category'],\n",
    "    title='Feedback Clusters Visualization',\n",
    "    labels={'x': 'UMAP 1', 'y': 'UMAP 2'},\n",
    "    color_discrete_map={-1: 'lightgray'}  # Noise points in gray\n",
    ")\n",
    "\n",
    "fig.update_traces(\n",
    "    marker=dict(size=10, line=dict(width=1, color='white')),\n",
    "    selector=dict(mode='markers')\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    width=900,\n",
    "    height=600,\n",
    "    plot_bgcolor='rgba(240,240,240,0.5)'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster size distribution\n",
    "cluster_sizes = df_filtered[df_filtered['cluster'] >= 0]['cluster'].value_counts().sort_index()\n",
    "\n",
    "fig_sizes = go.Figure(data=[\n",
    "    go.Bar(\n",
    "        x=[f'Cluster {i}' for i in cluster_sizes.index],\n",
    "        y=cluster_sizes.values,\n",
    "        text=cluster_sizes.values,\n",
    "        textposition='auto',\n",
    "    )\n",
    "])\n",
    "\n",
    "fig_sizes.update_layout(\n",
    "    title='Cluster Size Distribution',\n",
    "    xaxis_title='Cluster',\n",
    "    yaxis_title='Number of Feedback Entries',\n",
    "    showlegend=False,\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig_sizes.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analyze Cluster Themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_cluster(cluster_feedback, cluster_id):\n",
    "    \"\"\"\n",
    "    Use Gemini to analyze and describe a cluster based on sample feedback.\n",
    "    \"\"\"\n",
    "    # Take representative samples (up to 10)\n",
    "    samples = cluster_feedback[:min(10, len(cluster_feedback))]\n",
    "    \n",
    "    prompt = f\"\"\"Analyze the following {len(samples)} feedback comments that share similar themes:\n",
    "\n",
    "{chr(10).join([f'{i+1}. \"{sample}\"' for i, sample in enumerate(samples)])}\n",
    "\n",
    "Please provide:\n",
    "1. A brief 2-3 sentence description of the main theme or common characteristics\n",
    "2. Key topics or concerns mentioned (bullet points)\n",
    "3. The general tone (positive, negative, neutral, mixed)\n",
    "4. 2-3 actionable insights for improvement\n",
    "\n",
    "Keep the analysis concise and focused on patterns across all samples.\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model=LLM_MODEL,\n",
    "            contents=prompt,\n",
    "            config=types.GenerateContentConfig(\n",
    "                temperature=0.7,\n",
    "                max_output_tokens=500\n",
    "            )\n",
    "        )\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        return f\"Error analyzing cluster: {str(e)}\"\n",
    "\n",
    "# Analyze each cluster\n",
    "cluster_analyses = {}\n",
    "unique_clusters = sorted(set(cluster_labels))\n",
    "unique_clusters = [c for c in unique_clusters if c >= 0]  # Exclude noise\n",
    "\n",
    "print(\"Analyzing clusters...\\n\")\n",
    "for cluster_id in unique_clusters:\n",
    "    cluster_mask = df_filtered['cluster'] == cluster_id\n",
    "    cluster_feedback = df_filtered[cluster_mask]['feedback_text'].tolist()\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"CLUSTER {cluster_id} ({len(cluster_feedback)} entries)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Get analysis\n",
    "    analysis = analyze_cluster(cluster_feedback, cluster_id)\n",
    "    cluster_analyses[cluster_id] = {\n",
    "        'size': len(cluster_feedback),\n",
    "        'analysis': analysis,\n",
    "        'samples': cluster_feedback[:3]\n",
    "    }\n",
    "    \n",
    "    print(analysis)\n",
    "    \n",
    "    # Show sample feedback\n",
    "    print(\"\\nSample feedback:\")\n",
    "    for i, sample in enumerate(cluster_feedback[:3]):\n",
    "        print(f\"{i+1}. {sample[:100]}...\")\n",
    "    \n",
    "    time.sleep(1)  # Rate limiting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = Path('feedback_analysis_output')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save cluster assignments\n",
    "df_filtered.to_csv(output_dir / 'feedback_with_clusters.csv', index=False)\n",
    "print(f\"Saved cluster assignments to {output_dir / 'feedback_with_clusters.csv'}\")\n",
    "\n",
    "# Save cluster analyses\n",
    "with open(output_dir / 'cluster_analyses.json', 'w') as f:\n",
    "    json.dump(cluster_analyses, f, indent=2)\n",
    "print(f\"Saved cluster analyses to {output_dir / 'cluster_analyses.json'}\")\n",
    "\n",
    "# Generate summary report\n",
    "report = f\"\"\"# Feedback Analysis Report\n",
    "\n",
    "**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## Summary Statistics\n",
    "- Total feedback analyzed: {len(df_filtered)}\n",
    "- Number of clusters found: {n_clusters}\n",
    "- Noise points: {n_noise}\n",
    "- Average cluster size: {len(df_filtered[df_filtered['cluster'] >= 0]) / n_clusters:.1f}\n",
    "\n",
    "## Cluster Analyses\n",
    "\"\"\"\n",
    "\n",
    "for cluster_id, data in cluster_analyses.items():\n",
    "    report += f\"\\n### Cluster {cluster_id} (Size: {data['size']})\\n\\n\"\n",
    "    report += data['analysis'] + \"\\n\"\n",
    "\n",
    "# Save report\n",
    "with open(output_dir / 'analysis_report.md', 'w') as f:\n",
    "    f.write(report)\n",
    "print(f\"\\nSaved analysis report to {output_dir / 'analysis_report.md'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Advanced Analysis (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal analysis - how clusters change over time\n",
    "if 'date' in df_filtered.columns:\n",
    "    # Group by month and cluster\n",
    "    df_filtered['month'] = pd.to_datetime(df_filtered['date']).dt.to_period('M')\n",
    "    temporal_data = df_filtered[df_filtered['cluster'] >= 0].groupby(['month', 'cluster']).size().unstack(fill_value=0)\n",
    "    \n",
    "    # Create stacked area chart\n",
    "    fig_temporal = go.Figure()\n",
    "    \n",
    "    for cluster in temporal_data.columns:\n",
    "        fig_temporal.add_trace(go.Scatter(\n",
    "            x=temporal_data.index.astype(str),\n",
    "            y=temporal_data[cluster],\n",
    "            mode='lines',\n",
    "            stackgroup='one',\n",
    "            name=f'Cluster {cluster}'\n",
    "        ))\n",
    "    \n",
    "    fig_temporal.update_layout(\n",
    "        title='Cluster Distribution Over Time',\n",
    "        xaxis_title='Month',\n",
    "        yaxis_title='Number of Feedback Entries',\n",
    "        hovermode='x unified',\n",
    "        height=400\n",
    "    )\n",
    "    \n",
    "    fig_temporal.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-tabulation with categories (if available)\n",
    "if 'category' in df_filtered.columns:\n",
    "    # Create crosstab\n",
    "    crosstab = pd.crosstab(df_filtered['category'], df_filtered['cluster'], normalize='index') * 100\n",
    "    \n",
    "    # Create heatmap\n",
    "    fig_heatmap = px.imshow(\n",
    "        crosstab.values,\n",
    "        labels=dict(x=\"Cluster\", y=\"Category\", color=\"Percentage\"),\n",
    "        x=[f\"Cluster {i}\" if i >= 0 else \"Noise\" for i in crosstab.columns],\n",
    "        y=crosstab.index,\n",
    "        title=\"Category Distribution Across Clusters (%)\",\n",
    "        color_continuous_scale=\"Blues\"\n",
    "    )\n",
    "    \n",
    "    fig_heatmap.update_layout(height=400)\n",
    "    fig_heatmap.show()\n",
    "    \n",
    "    print(\"\\nKey insights:\")\n",
    "    for category in crosstab.index:\n",
    "        dominant_cluster = crosstab.loc[category].idxmax()\n",
    "        if dominant_cluster >= 0:\n",
    "            percentage = crosstab.loc[category, dominant_cluster]\n",
    "            print(f\"- {category} feedback is {percentage:.0f}% in Cluster {dominant_cluster}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusions and Next Steps\n",
    "\n",
    "### What We've Accomplished\n",
    "1. âœ… Converted text feedback into numerical embeddings\n",
    "2. âœ… Clustered similar feedback together\n",
    "3. âœ… Analyzed themes using AI\n",
    "4. âœ… Created visualizations\n",
    "5. âœ… Exported results for further analysis\n",
    "\n",
    "### Next Steps\n",
    "1. **Fine-tune parameters**: Adjust `min_cluster_size` and UMAP parameters\n",
    "2. **Add more data**: Include additional feedback sources\n",
    "3. **Track changes**: Run analysis periodically to track trends\n",
    "4. **Take action**: Use insights to improve your product/service\n",
    "5. **Automate**: Set up scheduled analysis pipelines\n",
    "\n",
    "### Tips for Production Use\n",
    "- Cache embeddings to avoid re-computing\n",
    "- Use batch processing for large datasets\n",
    "- Implement proper error handling\n",
    "- Monitor API costs\n",
    "- Consider fine-tuning embeddings for your domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save this notebook's configuration for reproducibility\n",
    "config = {\n",
    "    'analysis_date': datetime.now().isoformat(),\n",
    "    'parameters': {\n",
    "        'embedding_model': EMBEDDING_MODEL,\n",
    "        'llm_model': LLM_MODEL,\n",
    "        'min_feedback_length': MIN_FEEDBACK_LENGTH,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'min_cluster_size': MIN_CLUSTER_SIZE,\n",
    "        'umap_components': 50,\n",
    "        'umap_neighbors': 15\n",
    "    },\n",
    "    'results': {\n",
    "        'total_feedback': len(df_filtered),\n",
    "        'num_clusters': n_clusters,\n",
    "        'noise_points': n_noise\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(output_dir / 'analysis_config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(\"Analysis complete! ðŸŽ‰\")\n",
    "print(f\"\\nAll results saved to: {output_dir.absolute()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}